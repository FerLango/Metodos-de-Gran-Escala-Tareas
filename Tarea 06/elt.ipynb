{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELT\n",
    "\n",
    "Crea la base de datos en AWS Glue y carga los datos en ella a través de Athena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import boto3\n",
    "import awswrangler as wr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parámetros de configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "BUCKET_NAME = config[\"aws\"][\"bucket_name\"]\n",
    "DATABASE_NAME = config[\"aws\"][\"database_name\"]\n",
    "REGION = config[\"aws\"][\"region\"]\n",
    "ATHENA_OUTPUT = config[\"aws\"][\"athena_output\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear la base de datos en AWS Glue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea el cliente para glue\n",
    "glue = boto3.client('glue', region_name=REGION)\n",
    "\n",
    "database_input = {\n",
    "    'Name': DATABASE_NAME,\n",
    "    'Description': 'Base de datos de indicadores económicos'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La base de datos 'econ' ha sido creada exitosamente.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    glue.create_database(DatabaseInput=database_input)\n",
    "    print(f\"La base de datos '{DATABASE_NAME}' ha sido creada exitosamente.\")\n",
    "except ClientError as e:\n",
    "    # Si la base de datos ya existe\n",
    "    if e.response['Error']['Code'] == 'AlreadyExistsException':\n",
    "        print(f\"La base de datos '{DATABASE_NAME}' ya existe.\")\n",
    "    else:\n",
    "        print(f\"Error al crear la base de datos: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear las tablas dentro de la base de datos con Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La ruta de salida de Athena es: s3://itam-analytics-ferlango/athena_results\n"
     ]
    }
   ],
   "source": [
    "# Construir la ruta de salida de Athena\n",
    "athena_output = f's3://{BUCKET_NAME}/{ATHENA_OUTPUT}'\n",
    "print(f\"La ruta de salida de Athena es: {athena_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipo de cambio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabla 'tipo_de_cambio' creada en la base de datos econ\n"
     ]
    }
   ],
   "source": [
    "raw_tipo_de_cambio = f\"s3://{BUCKET_NAME}/{DATABASE_NAME}/raw/tipo_de_cambio/\"\n",
    "\n",
    "ddl_tipo_de_cambio = f\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS tipo_de_cambio (\n",
    "    timestamp date,\n",
    "    tipo_de_cambio double\n",
    ")\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\n",
    "WITH SERDEPROPERTIES ('field.delim' = ',')\n",
    "STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
    "LOCATION '{raw_tipo_de_cambio}'\n",
    "TBLPROPERTIES ('classification' = 'csv', \"skip.header.line.count\"=\"1\");\n",
    "\"\"\"\n",
    "\n",
    "wr.athena.start_query_execution(\n",
    "    sql=ddl_tipo_de_cambio,\n",
    "    database=DATABASE_NAME,\n",
    "    s3_output=athena_output\n",
    ")\n",
    "print(\"Tabla 'tipo_de_cambio' creada en la base de datos\", DATABASE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasa de interés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabla 'tasa_de_interes' creada en la base de datos econ\n"
     ]
    }
   ],
   "source": [
    "raw_tasa_de_interes = f\"s3://{BUCKET_NAME}/{DATABASE_NAME}/raw/tasa_de_interes/\"\n",
    "\n",
    "ddl_tasa_de_interes = f\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS tasa_de_interes (\n",
    "    timestamp date,\n",
    "    tasa_de_interes double\n",
    ")\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\n",
    "WITH SERDEPROPERTIES ('field.delim' = ',')\n",
    "STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
    "LOCATION '{raw_tasa_de_interes}'\n",
    "TBLPROPERTIES ('classification' = 'csv', \"skip.header.line.count\"=\"1\");\n",
    "\"\"\"\n",
    "\n",
    "wr.athena.start_query_execution(\n",
    "    sql=ddl_tasa_de_interes,\n",
    "    database=DATABASE_NAME,\n",
    "    s3_output=athena_output\n",
    ")\n",
    "\n",
    "print(\"Tabla 'tasa_de_interes' creada en la base de datos\", DATABASE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inflación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabla 'inflacion' creada en la base de datos econ\n"
     ]
    }
   ],
   "source": [
    "raw_inflacion = f\"s3://{BUCKET_NAME}/{DATABASE_NAME}/raw/inflacion/\"\n",
    "\n",
    "ddl_inflacion = f\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS inflacion (\n",
    "    timestamp date,\n",
    "    inflacion double\n",
    ")\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\n",
    "WITH SERDEPROPERTIES ('field.delim' = ',')\n",
    "STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
    "LOCATION '{raw_inflacion}'\n",
    "TBLPROPERTIES ('classification' = 'csv', \"skip.header.line.count\"=\"1\");\n",
    "\"\"\"\n",
    "\n",
    "wr.athena.start_query_execution(\n",
    "    sql=ddl_inflacion,\n",
    "    database=DATABASE_NAME,\n",
    "    s3_output=athena_output\n",
    ")\n",
    "\n",
    "print(\"Tabla 'inflacion' creada en la base de datos\", DATABASE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mensual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabla 'mensual' eliminada de la base de datos econ\n"
     ]
    }
   ],
   "source": [
    "processed_mensual = f\"s3://{BUCKET_NAME}/{DATABASE_NAME}/processed/mensual/\"\n",
    "\n",
    "# Eliminar los objetos en S3 para asegurar que se actualicen\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(BUCKET_NAME)\n",
    "bucket.objects.filter(Prefix=f\"{DATABASE_NAME}/processed/mensual/\").delete()\n",
    "\n",
    "# Elimina la tabla si existe para asegurar que se actualice\n",
    "drop_query = \"DROP TABLE IF EXISTS mensual\"\n",
    "wr.athena.start_query_execution(\n",
    "    sql=drop_query,\n",
    "    database=DATABASE_NAME,\n",
    "    s3_output=f\"s3://{BUCKET_NAME}/{ATHENA_OUTPUT}/\")\n",
    "\n",
    "print(\"Tabla 'mensual' eliminada de la base de datos\", DATABASE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabla 'mensual' creada en formato Parquet con datos consolidados mensualmente.\n"
     ]
    }
   ],
   "source": [
    "# Crea la tabla materializada con CTAS\n",
    "ddl_mensual = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS mensual\n",
    "WITH (\n",
    "    format = 'PARQUET',\n",
    "    external_location = '{processed_mensual}'\n",
    ") AS\n",
    "SELECT \n",
    "    i.timestamp AS date,\n",
    "    tc.avg_tipo_de_cambio AS tipo_de_cambio,\n",
    "    ti.avg_tasa_de_interes AS tasa_de_interes,\n",
    "    i.inflacion\n",
    "FROM (\n",
    "    SELECT timestamp, inflacion, year(timestamp) AS year, month(timestamp) AS month\n",
    "    FROM inflacion\n",
    ") i\n",
    "INNER JOIN (\n",
    "    SELECT year(timestamp) AS year, month(timestamp) AS month, AVG(tipo_de_cambio) AS avg_tipo_de_cambio\n",
    "    FROM tipo_de_cambio\n",
    "    GROUP BY year(timestamp), month(timestamp)\n",
    ") tc\n",
    "    ON i.year = tc.year AND i.month = tc.month\n",
    "INNER JOIN (\n",
    "    SELECT year(timestamp) AS year, month(timestamp) AS month, AVG(tasa_de_interes) AS avg_tasa_de_interes\n",
    "    FROM tasa_de_interes\n",
    "    GROUP BY year(timestamp), month(timestamp)\n",
    ") ti\n",
    "    ON i.year = ti.year AND i.month = ti.month\n",
    "\"\"\"\n",
    "\n",
    "wr.athena.start_query_execution(\n",
    "    sql=ddl_mensual,\n",
    "    database=DATABASE_NAME,\n",
    "    s3_output=f\"s3://{BUCKET_NAME}/{ATHENA_OUTPUT}/\"\n",
    ")\n",
    "\n",
    "print(\"Tabla 'mensual' creada en formato Parquet con datos consolidados mensualmente.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arquitectura",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
